import os
import re
import time
import tempfile
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urlparse, parse_qs, unquote
from openai import OpenAI

# OpenAI í´ë¼ì´ì–¸íŠ¸ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”)
client = OpenAI()

# ğŸ”¹ íŠ¸ìœ„í„° Bearer Token
TWITTER_BEARER_TOKEN = os.getenv("TWITTER_BEARER_TOKEN")

# -------------------------------
# ë‹¨ì¶• URL ì²˜ë¦¬
# -------------------------------
def resolve_redirect_url(url: str, timeout=3) -> str:
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://www.google.com/',
        }
        resp = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)
        final_url = resp.url
        if resp.status_code >= 400 or final_url == url:
            resp = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
            final_url = resp.url
        # ì—ëŸ¬ í˜ì´ì§€ ê°ì§€
        error_indicators = ['error.html','blocked','forbidden','access-denied','se-cu.com/ndsoft','404','403','500']
        for indicator in error_indicators:
            if indicator in final_url.lower():
                print(f"ğŸš« ì—ëŸ¬ í˜ì´ì§€ ê°ì§€: {final_url} â†’ ì›ë³¸ URL ì‚¬ìš©")
                return url
        # Naver bridge ì²˜ë¦¬
        if "link.naver.com/bridge" in final_url:
            qs = parse_qs(urlparse(final_url).query)
            if "url" in qs:
                decoded = unquote(qs["url"][0])
                print("ğŸ”¹ Bridge URL â†’ ì‹¤ì œ URL:", decoded)
                return decoded
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë„ë©”ì¸ ë³€ê²½
        original_domain = urlparse(url).netloc
        final_domain = urlparse(final_url).netloc
        if original_domain != final_domain and not any(short in original_domain for short in ['bit.ly', 'tinyurl', 'naver.me', 't.co']):
            print(f"ğŸš« ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë„ë©”ì¸ ë³€ê²½: {original_domain} â†’ {final_domain}, ì›ë³¸ URL ì‚¬ìš©")
            return url
        return final_url
    except Exception as e:
        print(f"âš ï¸ ë¦¬ë‹¤ì´ë ‰íŠ¸ URL í•´ì„ ì‹¤íŒ¨: {e}")
        return url

# -------------------------------
# requests + BeautifulSoup ë³¸ë¬¸ ì¶”ì¶œ
# -------------------------------
def try_requests_first(url: str, timeout=5) -> dict | None:
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        resp = requests.get(url, headers=headers, timeout=timeout)
        if not resp.ok:
            print(f"âš ï¸ HTTP ìƒíƒœ ì½”ë“œ: {resp.status_code}")
            return None
        if "text/html" not in resp.headers.get("Content-Type", ""):
            print("âš ï¸ HTML ì½˜í…ì¸ ê°€ ì•„ë‹˜")
            return None
        soup = BeautifulSoup(resp.text, "html.parser")
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = ['h1','.article-title','.news-title','.post-title','title']
        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem and elem.get_text().strip():
                title = elem.get_text().strip()
                break
        if not title and soup.title:
            title = soup.title.get_text().strip()
        # ë³¸ë¬¸ ì¶”ì¶œ
        body_text = ""
        article_selectors = ['article','.article-content','.article-body','.news-content','.post-content','.content','#article-view-content-div','.view-content','.article_txt','.news_txt']
        for selector in article_selectors:
            article_elem = soup.select_one(selector)
            if article_elem:
                for unwanted in article_elem.find_all(['script','style','iframe','ins']):
                    unwanted.decompose()
                for unwanted_class in article_elem.find_all(class_=['ad','advertisement','related','recommend']):
                    unwanted_class.decompose()
                body_text = article_elem.get_text(separator='\n').strip()
                if len(body_text) > 100:
                    break
        if not body_text or len(body_text) < 100:
            for unwanted in soup.find_all(['script','style','nav','header','footer','aside','iframe']):
                unwanted.decompose()
            body_text = soup.get_text(separator='\n')
        lines = [line.strip() for line in body_text.split('\n') if line.strip()]
        body_text = '\n'.join(lines)
        print(f"ğŸ“„ ì¶”ì¶œëœ ì œëª©: {title}")
        print(f"ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {len(body_text)} ê¸€ì")
        print(f"ğŸ” ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {body_text[:200]}...")
        return {"title": title, "body": body_text[:4000]}
    except Exception as e:
        print(f"âš ï¸ requestsë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

# -------------------------------
# Selenium ë³¸ë¬¸ ì¶”ì¶œ
# -------------------------------
def fetch_with_selenium(url: str, wait_time=5) -> dict:
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-default-apps")
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--disable-notifications")
    options.add_argument("--mute-audio")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--disable-web-security")
    options.add_argument("--allow-running-insecure-content")
    options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)
    temp_dir = tempfile.mkdtemp()
    options.add_argument(f"--user-data-dir={temp_dir}")
    driver = webdriver.Chrome(options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    title = ""
    body_text = ""
    try:
        print(f"ğŸŒ Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë“œ: {url}")
        driver.get(url)
        WebDriverWait(driver, wait_time).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(3)
        current_url = driver.current_url
        if 'error.html' in current_url or 'blocked' in current_url:
            print(f"ğŸš« ì—ëŸ¬ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨: {current_url}")
            return {"title": "ì ‘ê·¼ ì°¨ë‹¨ë¨", "body": "ì‚¬ì´íŠ¸ì—ì„œ ë´‡ ì ‘ê·¼ì„ ì°¨ë‹¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."}
        title = driver.title.strip()
        print(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {title}")
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸
        if "blog.naver.com" in url:
            try:
                WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, "mainFrame")))
                iframe = driver.find_element(By.ID, "mainFrame")
                driver.switch_to.frame(iframe)
                time.sleep(1)
                try:
                    article = driver.find_element(By.CLASS_NAME, "se-main-container")
                except:
                    article = driver.find_element(By.ID, "postViewArea")
                body_text = article.text.strip()
                print(f"ğŸ“ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ê¸¸ì´: {len(body_text)}")
            except Exception as e:
                print(f"âš ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ iframe ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        else:
            article_selectors = ["#article-view-content-div",".article_txt",".news_txt","article",".article-content",".article-body",".news-content",".post-content",".content",".view-content",".article_view",".news_view"]
            for selector in article_selectors:
                try:
                    if selector.startswith('.'):
                        elements = driver.find_elements(By.CLASS_NAME, selector[1:])
                    elif selector.startswith('#'):
                        elements = driver.find_elements(By.ID, selector[1:])
                    else:
                        elements = driver.find_elements(By.TAG_NAME, selector)
                    if elements:
                        body_text = elements[0].text.strip()
                        if len(body_text) > 100:
                            print(f"ğŸ“ ì„ íƒì '{selector}'ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(body_text)}ê¸€ì")
                            break
                except:
                    continue
            if not body_text or len(body_text) < 100:
                print("ğŸ” í˜ì´ì§€ ì†ŒìŠ¤ ë¶„ì„ ì¤‘...")
                page_source = driver.page_source
                if len(page_source) < 1000:
                    print("âš ï¸ í˜ì´ì§€ ì†ŒìŠ¤ê°€ ë„ˆë¬´ ì§§ìŒ - ì°¨ë‹¨ ê°€ëŠ¥ì„±")
                    return {"title": title or "ì°¨ë‹¨ë¨", "body": "í˜ì´ì§€ ë‚´ìš©ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ"}
                soup = BeautifulSoup(page_source, 'html.parser')
                for unwanted in soup.find_all(['script','style','nav','header','footer','aside']):
                    unwanted.decompose()
                body_text = soup.get_text(separator='\n')
                lines = [line.strip() for line in body_text.split('\n') if line.strip() and len(line.strip()) > 10]
                body_text = '\n'.join(lines)
                print(f"ğŸ“ ì „ì²´ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ: {len(body_text)}ê¸€ì")
    except Exception as e:
        print(f"âŒ Selenium ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    finally:
        driver.quit()
    return {"title": title, "body": body_text[:4000]}

# -------------------------------
# íŠ¸ìœ„í„° ë³¸ë¬¸ ì¶”ì¶œ
# -------------------------------
def fetch_twitter_content(url: str) -> dict | None:
    try:
        match = re.search(r"(?:twitter|x)\.com/[^/]+/status/(\d+)", url)
        if not match:
            print("âš ï¸ íŠ¸ìœ„í„° URLì—ì„œ tweet_id ì¶”ì¶œ ì‹¤íŒ¨")
            return None
        tweet_id = match.group(1)
        if not TWITTER_BEARER_TOKEN:
            print("âš ï¸ TWITTER_BEARER_TOKEN í™˜ê²½ë³€ìˆ˜ ì—†ìŒ")
            return None
        headers = {"Authorization": f"Bearer {TWITTER_BEARER_TOKEN}","User-Agent":"v2TweetLookupPython"}
        api_url = f"https://api.twitter.com/2/tweets/{tweet_id}?expansions=author_id&tweet.fields=created_at,lang&user.fields=username,name"
        resp = requests.get(api_url, headers=headers, timeout=5)
        if not resp.ok:
            print(f"âš ï¸ íŠ¸ìœ„í„° API ì˜¤ë¥˜: {resp.status_code} {resp.text}")
            return None
        data = resp.json()
        tweet_data = data.get("data", {})
        includes = data.get("includes", {})
        text = tweet_data.get("text", "")
        created_at = tweet_data.get("created_at", "")
        author = ""
        if "users" in includes:
            user = includes["users"][0]
            author = f"{user.get('name')} (@{user.get('username')})"
        title = f"íŠ¸ìœ„í„° ê¸€ by {author} ({created_at})"
        body = text.strip()
        print(f"ğŸ¦ íŠ¸ìœ„í„° ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(body)}ì")
        return {"title": title, "body": body}
    except Exception as e:
        print(f"âŒ íŠ¸ìœ„í„° ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

# -------------------------------
# ê¸°ì‚¬/ì›¹ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
# -------------------------------
def fetch_article_content(url: str) -> dict:
    # íŠ¸ìœ„í„° URL ìš°ì„  ì²˜ë¦¬
    if "twitter.com" in url or "x.com" in url:
        tw = fetch_twitter_content(url)
        if tw:
            return tw
    original_url = url
    if "blog.naver.com" in url and not url.startswith("https://m."):
        url = url.replace("https://blog.naver.com","https://m.blog.naver.com")
    print(f"ğŸ” 1ë‹¨ê³„: requestsë¡œ ì‹œë„ - {url}")
    article = try_requests_first(url)
    if article and article["body"].strip() and len(article["body"]) > 100:
        print("âœ… requestsë¡œ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œ")
        return article
    if url != original_url:
        print(f"ğŸ” 1-2ë‹¨ê³„: ì›ë³¸ URLë¡œ requests ì¬ì‹œë„ - {original_url}")
        article = try_requests_first(original_url)
        if article and article["body"].strip() and len(article["body"]) > 100:
            print("âœ… ì›ë³¸ URLë¡œ requests ì„±ê³µ")
            return article
    print("ğŸ” 2ë‹¨ê³„: Seleniumìœ¼ë¡œ ì¬ì‹œë„")
    selenium_result = fetch_with_selenium(original_url)
    if (not selenium_result["body"] or len(selenium_result["body"]) < 100) and url != original_url:
        print("ğŸ” 2-2ë‹¨ê³„: ë³€í™˜ëœ URLë¡œ Selenium ì¬ì‹œë„")
        selenium_result = fetch_with_selenium(url)
    return selenium_result

# -------------------------------
# GPT ìš”ì•½
# -------------------------------
def summarize_text(article: dict) -> str:
    title = article.get("title","")
    body = article.get("body","")
    if not body or len(body.strip()) < 50:
        return "- ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨\n- ì‚¬ì´íŠ¸ ì ‘ê·¼ ì œí•œ ê°€ëŠ¥ì„±\n- Selenium ì¬ì‹œë„ í•„ìš”\n- "
    prompt = f"""
ì œëª©ê³¼ ë³¸ë¬¸ì„ ë³´ê³ 
ê¸°ì‚¬ ë‚´ìš©ì„ í•µì‹¬ë§Œ ìš”ì•½í•´ì¤˜.
ëˆ„ê°€ ë­˜ ì–´ë–»ê²Œ í–ˆëŠ”ì§€, ì¥ì†Œì™€ ì£¼ì²´, ëŒ€ìƒ êµ¬ì²´ì ìœ¼ë¡œ ë§í•´ì¤˜ì•¼
ë‚´ìš©ì˜ í˜¼ë™ì´ ì—†ìŒ.
ê¸°ìŠ¹ì „ê²°ì´ë©´ ë” ì¢‹ê³ , ì •í™•í•˜ê²Œ ìš”ì•½í•´ì¤˜.

- ê° í•­ëª©ì€ 28ì ì´ë‚´, ìŒìŠ´ì²´
- ì¶œë ¥ì€ 'âœ… ìš”ì•½'ì„ ì œì¼ ì²«ì¤„ë¡œ ì‹œì‘í•˜ê³ , ê·¸ ì•„ë˜ë¡œ ì´ 6ì¤„ë¡œ ê° ì—´ ì‹œì‘ë§ˆë‹¤ '-'ë¥¼ ë¶™ì—¬ì£¼ê³  ìì—°ìŠ¤ëŸ½ê³  ê¹”ë”í•˜ê²Œ ìš”ì•½

ì œëª©:
\"\"\"{title}\"\"\"

ë³¸ë¬¸:
\"\"\"{body}\"\"\"
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=300
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT ìš”ì•½ ì‹¤íŒ¨: {e}")
        return f"âœ… ìš”ì•½\n- ì œëª©: {title[:30]}\n- ë³¸ë¬¸ ê¸¸ì´: {len(body)}ê¸€ì\n- GPT ìš”ì•½ ì‹¤íŒ¨\n- ì§ì ‘ í™•ì¸ í•„ìš”\n- \n- "

# -------------------------------
# ë©”ì‹œì§€ì—ì„œ URL ì°¾ì•„ ìš”ì•½
# -------------------------------
def url_summary(chat) -> str | None:
    if isinstance(chat,str):
        msg = chat
    else:
        msg = chat.message.msg
    url_pattern = re.compile(r'https?://[^\s]+')
    url_match = url_pattern.search(msg)
    if url_match:
        url = url_match.group(0)
        print("âœ… ë©”ì‹œì§€ì—ì„œ URL ë°œê²¬:", url)
        resolved_url = resolve_redirect_url(url)
        print("ğŸ”¹ ì‹¤ì œ URL:", resolved_url)
        try:
            article = fetch_article_content(resolved_url)
            summary = summarize_text(article)
            print(f"\nğŸ”¹ ì œëª©: {article.get('title','')}")
            print("ğŸ”¹ ìš”ì•½ ê²°ê³¼:")
            print(summary)
            return summary
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"âœ… ì˜¤ë¥˜\n- URL ì²˜ë¦¬ ì‹¤íŒ¨\n- ì˜¤ë¥˜: {str(e)[:30]}\n- ì‚¬ì´íŠ¸ ì ‘ê·¼ ì œí•œ ê°€ëŠ¥ì„±\n- ìˆ˜ë™ í™•ì¸ í•„ìš”\n- \n- "
    else:
        print("âŒ ë©”ì‹œì§€ì— URLì´ ì—†ìŠµë‹ˆë‹¤:", msg)
        return None

# -------------------------------
# í…ŒìŠ¤íŠ¸ìš©
# -------------------------------
def test_url(url: str):
    return url_summary(url)
